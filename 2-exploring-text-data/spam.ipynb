{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam or Ham (Working Title)\n",
    "Lab Assignment Two: Exploring Text Data\n",
    "\n",
    "**_Jake Oien, Seung Ki Lee, Jenn Le_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Here, we'll import the data, remove unwanted columns(cause this data has 3 empty columns for some reason,\n",
    "# and rename the columns to be more descriptive\n",
    "data = pd.read_csv(\"./spam.csv\", encoding='latin-1')\n",
    "data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "data = data.rename(columns={\"v1\":\"label\", \"v2\":\"text\"})\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Encoding\n",
    "\n",
    "### Read in Data as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requests for handling url\n",
    "import requests\n",
    "import warnings\n",
    "\n",
    "#for testing\n",
    "TEST = False\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "\n",
    "# #read in data as string\n",
    "ds = data.to_string(index=True)\n",
    "\n",
    "if TEST:\n",
    "    print(type(ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Data Quality\n",
    "\n",
    "To clean up the data set, we've analyzed what words were meaningless in the context of constituting a message. First major filler we've noticed was the markdown tags. We concluded words such as **& lt;#& gt;** are used for formatting purposes and not for anything pertinent to the meaning of the text. Also, we did not come across any words which started with '&' and ended with ';' which wasn't a markdown tag, so the probability of removing important data seems very low. Aside from this, generally accepted stopwords embeded in the sklearn were removed as they work to give context but not meaning.\n",
    "\n",
    "We also noticed that callback numbers, website urls and email addresses are usually unique or sparse in appearance, however they were strong indicators of spam. For purposes of further analysis we did not remove these words, but rather we replaced them for collective verifiers such as \"replaced_callback_number\" or \"replaced_url\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify data quality\n",
    "import re\n",
    "\n",
    "#remove irrelavant words : markdown tags\n",
    "markdown_tag = r\"([\\&].+[\\;])\"\n",
    "ds = re.sub(markdown_tag,' ',ds)\n",
    "\n",
    "# email_address = r\"^([\\w|-]+\\@[\\w]+\\.[\\w]+)$\"\n",
    "# ds = re.sub(email_address,\"replaced_email\",ds)\n",
    "\n",
    "# #cited from https://www.regexpal.com/93652\n",
    "# website_url = r\"^(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?$\"\n",
    "# ds = re.sub(website_url,\"replaced_url\",ds)\n",
    "\n",
    "# #replace sparse but indicative words : callback numbers, email addresses, urls\n",
    "# #cited from https://www.regextester.com/17\n",
    "# callback_number = r\"^(?:(?:\\+?1\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?$\" \n",
    "# ds = re.sub(website_url, \"replaced_phone_number\", ds)\n",
    "\n",
    "if TEST:\n",
    "    print (ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert the data into a sparse encoded bag-of-word representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hi        130\n",
       "sorry     136\n",
       "come      145\n",
       "know      152\n",
       "good      162\n",
       "got       165\n",
       "ll        184\n",
       "ur        203\n",
       "just      218\n",
       "ok        246\n",
       "spam      748\n",
       "ham      4825\n",
       "dtype: int64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#create bag of words\n",
    "count_vector = CountVectorizer(stop_words='english')\n",
    "bag_of_words = count_vector.fit_transform([ds])\n",
    "\n",
    "#put word counts in pd.DataFrame\n",
    "df = pd.DataFrame(data=bag_of_words.toarray(), columns=count_vector.get_feature_names())\n",
    "\n",
    "df.sum().sort_values()[-12:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from Ham and Spam which are classifiers in the dataset, top 10 word in the dataset are typical \"responses\" in a casual texting environment such as \"ok,\" \"good,\" \"hi,\" \"sorry,\" or \"good.\"\n",
    "\n",
    "On the other hand, we also observed causative verb of \"got,\" and \"ur,\" a shorthand term for \"your\" or \"you are.\" Adverb \"just\" listed no.2 of the entire words as it has various usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert data into sparse encoded tr-idf representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hi       0.026140\n",
       "sorry    0.027346\n",
       "come     0.029156\n",
       "know     0.030563\n",
       "good     0.032574\n",
       "got      0.033177\n",
       "ll       0.036997\n",
       "ur       0.040818\n",
       "just     0.043834\n",
       "ok       0.049464\n",
       "spam     0.150403\n",
       "ham      0.970178\n",
       "dtype: float64"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Create tfidf\n",
    "tfidf_vector = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vector.fit_transform([ds])\n",
    "\n",
    "#build pd.DataFrame\n",
    "tfidf_df = pd.DataFrame(data=tfidf_matrix.toarray(), columns=tfidf_vector.get_feature_names())\n",
    "\n",
    "#tfidf_df.head(n=10)\n",
    "tfidf_df.sum().sort_values()[-12:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TF-IDF representation, the pool of top words stayed the same. Not only that, the ordering of frequency maintained the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
